{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Literature Mapper","text":"<p>Transform a folder of PDFs into a structured, queryable Knowledge Graph.</p> <p>Literature Mapper is an AI-powered Python library that extracts typed claims (findings, methods, limitations) with confidence scores, enriches papers with OpenAlex citation data, and provides LLM agents that synthesize answers strictly from your corpus\u2014no hallucinated citations, no external knowledge bleed.</p> <ul> <li> <p> Knowledge Graph Extraction</p> <p>Extract typed nodes and semantic edges (SUPPORTS, CONTRADICTS, EXTENDS) from your papers. The foundation for everything else.</p> </li> <li> <p> Semantic Search</p> <p>Find evidence by meaning, not just keywords. Uses MMR reranking for diverse, high-quality retrieval.</p> </li> <li> <p> Temporal Analysis</p> <p>Track concept trends, detect rising/declining topics, and identify revival patterns across publication years.</p> </li> <li> <p> Gap Detection (Ghost Hunting)</p> <p>Find papers and authors frequently cited by your corpus but missing from it.</p> </li> <li> <p> Bounded Synthesis</p> <p>RAG agents that answer questions and validate hypotheses using only your corpus, with full citation provenance.</p> </li> <li> <p> Intellectual Genealogy</p> <p>Trace the lineage of ideas. Identify seminal papers that spawned entire sub-fields and visualize the inheritance of concepts.</p> </li> <li> <p> Graph Export</p> <p>GEXF export for Gephi: co-authorship networks, concept maps, and paper similarity graphs.</p> </li> </ul>"},{"location":"#what-is-literature-mapper","title":"What is Literature Mapper?","text":"<p>Literature Mapper is designed for researchers working with curated collections of 10\u2013500 papers. It systematically:</p> <ol> <li>Processes PDFs \u2014 Extracts text, identifies metadata, and builds a knowledge graph</li> <li>Enriches with citations \u2014 Fetches citation counts and references from OpenAlex</li> <li>Enables discovery \u2014 Semantic search, ghost hunting, temporal trends</li> <li>Synthesizes knowledge \u2014 AI agents answer questions grounded strictly in your corpus</li> </ol>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install literature-mapper\n</code></pre> <p>Set your API key:</p> <pre><code>export GEMINI_API_KEY=\"your_api_key_here\"\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from literature_mapper import LiteratureMapper\n\n# Initialize with your PDF folder\nmapper = LiteratureMapper(\"./my_research\")\n\n# Process all PDFs\nmapper.process_new_papers(recursive=True)\n\n# Synthesize an answer from your corpus\nanswer = mapper.synthesize_answer(\n    \"What are the main debates about network structure?\"\n)\nprint(answer)\n</code></pre>"},{"location":"#frequently-asked-questions","title":"Frequently Asked Questions","text":"What is a Knowledge Graph in Literature Mapper? <p>A Knowledge Graph (KG) is a structured representation of your academic literature. Each paper becomes a node, connected to concepts, findings, methods, and limitations it discusses. Edges represent relationships like SUPPORTS, CONTRADICTS, or EXTENDS.</p> How does Ghost Hunting work? <p>Ghost Hunting uses your corpus's citation data to find structural gaps. \"Bibliographic ghosts\" are papers frequently cited by your corpus but not included. \"Author ghosts\" are researchers cited often but not represented.</p> What models does Literature Mapper use? <p>Literature Mapper uses Google's Gemini models for text extraction and synthesis, and <code>text-embedding-004</code> for semantic embeddings. All processing happens via the Gemini API.</p> Can I use Literature Mapper without an API key? <p>No, a Gemini API key is required for processing PDFs and synthesis. However, once papers are processed, many analysis features (search, ghost hunting, export) work offline.</p> How much does it cost to process papers? <p>Approximately $0.50 USD for 50 papers via the Gemini API. Costs vary based on paper length.</p> Does Google train on my PDFs? <p>No. Literature Mapper uses the paid tier of the Gemini API (via your API key), which has a strict zero-data-retention policy. Your data is not used to train models.</p> Does it work with non-English papers? <p>Yes. Because Gemini is natively multilingual, Literature Mapper can process, index, and synthesize answers from papers in German, French, Chinese, Japanese, and many other languages without translation steps.</p> How do I see the graph? <p>You can export your Knowledge Graph to GEXF format using the <code>viz</code> command (<code>literature-mapper viz ./my_research</code>). This file can be opened in Gephi, a free open-source visualization tool, to explore connection clusters and intellectual genealogies.</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete method reference for the Literature Mapper Python API.</p>"},{"location":"api/#literaturemapper","title":"LiteratureMapper","text":"<p>The main entry point for corpus management.</p> <pre><code>from literature_mapper import LiteratureMapper\n\nmapper = LiteratureMapper(corpus_path, model_name=\"gemini-3-flash-preview\")\n</code></pre>"},{"location":"api/#processing","title":"Processing","text":"Method Parameters Returns Description <code>process_new_papers()</code> <code>recursive=False</code> <code>ProcessingResult</code> Process unprocessed PDFs <code>update_citations()</code> <code>email=None</code> <code>None</code> Fetch OpenAlex citation data"},{"location":"api/#retrieval","title":"Retrieval","text":"Method Parameters Returns Description <code>get_all_analyses()</code> \u2014 <code>DataFrame</code> All papers as DataFrame <code>get_paper_by_id()</code> <code>paper_id: int</code> <code>dict</code> Single paper details <code>search_papers()</code> <code>column: str, query: str</code> <code>DataFrame</code> Keyword search <code>search_corpus()</code> <code>query, semantic, use_enhanced, min_year, max_year, node_types, limit</code> <code>list[dict]</code> Semantic search <code>get_statistics()</code> \u2014 <code>CorpusStats</code> Corpus counts"},{"location":"api/#normalization","title":"Normalization","text":"Method Parameters Returns Description <code>normalize_authors()</code> <code>mappings: dict</code> <code>int</code> Merge author aliases <code>normalize_concepts()</code> <code>mappings: dict</code> <code>int</code> Merge concept aliases <code>get_canonical_author()</code> <code>alias: str</code> <code>str</code> Resolve author alias <code>get_canonical_concept()</code> <code>alias: str</code> <code>str</code> Resolve concept alias"},{"location":"api/#temporal","title":"Temporal","text":"Method Parameters Returns Description <code>get_concept_timeline()</code> <code>concept=None, top_n=10</code> <code>DataFrame</code> Concept temporal data <code>compute_temporal_stats()</code> \u2014 <code>None</code> Compute trend statistics <code>get_trending_concepts()</code> <code>direction, limit</code> <code>DataFrame</code> Rising/declining concepts <code>detect_concept_eras()</code> <code>gap: int</code> <code>DataFrame</code> Revival detection"},{"location":"api/#genealogy","title":"Genealogy","text":"Method Parameters Returns Description <code>build_genealogy()</code> <code>verbose=False</code> <code>dict</code> Infer paper relationships <code>find_contradictions()</code> <code>concept=None</code> <code>DataFrame</code> CHALLENGES edges <code>get_argument_evolution()</code> <code>concept: str</code> <code>DataFrame</code> Temporal relationship trace"},{"location":"api/#synthesis","title":"Synthesis","text":"Method Parameters Returns Description <code>synthesize_answer()</code> <code>question, year_range=None, limit=10</code> <code>str</code> RAG answer synthesis <code>validate_hypothesis()</code> <code>hypothesis: str, limit=10</code> <code>dict</code> Hypothesis evaluation"},{"location":"api/#export","title":"Export","text":"Method Parameters Returns Description <code>export_to_csv()</code> <code>output_path: str</code> <code>None</code> Export to CSV"},{"location":"api/#ghosthunter","title":"GhostHunter","text":"<p>Gap detection for missing papers and authors.</p> <pre><code>from literature_mapper.ghosts import GhostHunter\n\nhunter = GhostHunter(mapper)\n</code></pre> Method Parameters Returns Description <code>find_bibliographic_ghosts()</code> <code>threshold=3</code> <code>DataFrame</code> Missing cited papers <code>find_missing_authors()</code> <code>threshold=5</code> <code>DataFrame</code> Missing cited authors"},{"location":"api/#corpusanalyzer","title":"CorpusAnalyzer","text":"<p>Corpus-level statistics and analytics.</p> <pre><code>from literature_mapper.analysis import CorpusAnalyzer\n\nanalyzer = CorpusAnalyzer(corpus_path)\n</code></pre> Method Parameters Returns Description <code>get_year_distribution()</code> \u2014 <code>DataFrame</code> Papers per year <code>get_top_authors()</code> <code>limit=10</code> <code>DataFrame</code> Most prolific authors <code>get_top_concepts()</code> <code>limit=10</code> <code>DataFrame</code> Most frequent concepts <code>find_hub_papers()</code> <code>limit=10</code> <code>DataFrame</code> Most referenced papers"},{"location":"api/#visualization","title":"Visualization","text":"<pre><code>from literature_mapper.viz import export_to_gexf\n\nexport_to_gexf(corpus_path, output_path, mode=\"semantic\", threshold=0.05)\n</code></pre> Parameter Type Default Description <code>corpus_path</code> <code>str</code> \u2014 Path to corpus directory <code>output_path</code> <code>str</code> \u2014 Output <code>.gexf</code> file path <code>mode</code> <code>str</code> <code>\"semantic\"</code> Graph type: <code>semantic</code>, <code>authors</code>, <code>concepts</code>, <code>river</code>, <code>similarity</code> <code>threshold</code> <code>float</code> <code>0.05</code> Minimum edge weight (for <code>similarity</code> mode)"},{"location":"api/#data-classes","title":"Data Classes","text":""},{"location":"api/#processingresult","title":"ProcessingResult","text":"<pre><code>@dataclass\nclass ProcessingResult:\n    processed: int    # Number of successfully processed PDFs\n    skipped: int      # Already in database\n    failed: int       # Processing errors\n</code></pre>"},{"location":"api/#corpusstats","title":"CorpusStats","text":"<pre><code>@dataclass\nclass CorpusStats:\n    total_papers: int\n    total_authors: int\n    total_concepts: int\n</code></pre>"},{"location":"cli/","title":"CLI Reference","text":"<p>Literature Mapper provides a complete command-line interface for managing your research corpus.</p>"},{"location":"cli/#core-workflow","title":"Core Workflow","text":""},{"location":"cli/#process-pdfs","title":"Process PDFs","text":"<p>Extract text and build the Knowledge Graph:</p> <pre><code>literature-mapper process ./my_research --recursive\n</code></pre> Option Description <code>--recursive</code> Process PDFs in subdirectories"},{"location":"cli/#fetch-citations","title":"Fetch Citations","text":"<p>Enrich your corpus with OpenAlex citation data:</p> <pre><code>literature-mapper citations ./my_research --email you@example.com\n</code></pre> Option Description <code>--email</code> Your email (optional, for faster rate limits)"},{"location":"cli/#view-status","title":"View Status","text":"<p>Check corpus health and statistics:</p> <pre><code>literature-mapper status ./my_research\n</code></pre>"},{"location":"cli/#search-analysis","title":"Search &amp; Analysis","text":""},{"location":"cli/#synthesize-answers","title":"Synthesize Answers","text":"<p>Ask a research question:</p> <pre><code>literature-mapper synthesize ./my_research \"What is the impact of X on Y?\"\n</code></pre>"},{"location":"cli/#validate-hypotheses","title":"Validate Hypotheses","text":"<p>Test a claim against your corpus:</p> <pre><code>literature-mapper validate ./my_research \"X causes Y.\"\n</code></pre>"},{"location":"cli/#find-hub-papers","title":"Find Hub Papers","text":"<p>Identify the most referenced papers:</p> <pre><code>literature-mapper hubs ./my_research --limit 10\n</code></pre>"},{"location":"cli/#corpus-statistics","title":"Corpus Statistics","text":"<p>View comprehensive stats:</p> <pre><code>literature-mapper stats ./my_research\n</code></pre>"},{"location":"cli/#temporal-analysis","title":"Temporal Analysis","text":"<p>Run <code>temporal</code> first</p> <p>Temporal commands require computing stats first with <code>literature-mapper temporal</code>.</p> <pre><code># Compute temporal stats (run this first!)\nliterature-mapper temporal ./my_research\n\n# View rising concepts\nliterature-mapper trends ./my_research --direction rising\n\n# View declining concepts\nliterature-mapper trends ./my_research --direction declining\n\n# Analyze a specific concept's trajectory\nliterature-mapper trajectory \"homophily\" ./my_research\n\n# Detect concept eras (revivals after gaps)\nliterature-mapper eras ./my_research --gap 5\n</code></pre>"},{"location":"cli/#gap-detection-ghost-hunting","title":"Gap Detection (Ghost Hunting)","text":"<p>Find missing papers and authors:</p> <pre><code># Find missing papers (bibliographic ghosts)\nliterature-mapper ghosts ./my_research --mode bibliographic --threshold 3\n\n# Find missing authors\nliterature-mapper ghosts ./my_research --mode authors --threshold 5\n</code></pre> Option Description <code>--mode</code> <code>bibliographic</code> (papers) or <code>authors</code> <code>--threshold</code> Minimum citations to be considered a \"ghost\""},{"location":"cli/#export-visualization","title":"Export &amp; Visualization","text":""},{"location":"cli/#gexf-export","title":"GEXF Export","text":"<pre><code># Semantic knowledge graph (default)\nliterature-mapper viz ./my_research -o graph.gexf\n\n# Co-authorship network\nliterature-mapper viz ./my_research -o authors.gexf --mode authors\n\n# Concept co-occurrence\nliterature-mapper viz ./my_research -o concepts.gexf --mode concepts\n\n# Paper similarity\nliterature-mapper viz ./my_research -o similar.gexf --mode similarity --threshold 0.3\n</code></pre> Mode Description <code>semantic</code> Full knowledge graph <code>authors</code> Co-authorship network <code>concepts</code> Topic co-occurrence <code>river</code> Concepts with temporal data <code>similarity</code> Paper similarity (Jaccard)"},{"location":"cli/#csv-export","title":"CSV Export","text":"<pre><code>literature-mapper export ./my_research output.csv\n</code></pre>"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"Variable Purpose Default <code>GEMINI_API_KEY</code> Required. Google AI key None <code>LITERATURE_MAPPER_MODEL</code> Default model <code>gemini-3-flash-preview</code> <code>LITERATURE_MAPPER_MAX_FILE_SIZE</code> Max PDF size (bytes) <code>52428800</code> (50 MB) <code>LITERATURE_MAPPER_BATCH_SIZE</code> PDFs per batch <code>10</code> <code>LITERATURE_MAPPER_LOG_LEVEL</code> Log level <code>INFO</code> <code>LITERATURE_MAPPER_VERBOSE</code> Debug mode <code>false</code>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get Literature Mapper running in 5 minutes.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"Requirement Details Python 3.10 or newer API Key <code>GEMINI_API_KEY</code> environment variable (get one) Corpus A folder containing PDF files"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install literature-mapper\n</code></pre> <p>Configure your API key:</p> Linux/macOSWindows (PowerShell) <pre><code>export GEMINI_API_KEY=\"your_api_key_here\"\n</code></pre> <pre><code>$env:GEMINI_API_KEY=\"your_api_key_here\"\n</code></pre>"},{"location":"quickstart/#basic-workflow","title":"Basic Workflow","text":""},{"location":"quickstart/#1-initialize-the-mapper","title":"1. Initialize the Mapper","text":"<pre><code>from literature_mapper import LiteratureMapper\n\n# Point to your PDF folder (creates corpus.db automatically)\nmapper = LiteratureMapper(\"./my_research\")\n</code></pre>"},{"location":"quickstart/#2-process-your-pdfs","title":"2. Process Your PDFs","text":"<pre><code># Process all PDFs (incremental\u2014skips already-processed files)\nresult = mapper.process_new_papers(recursive=True)\n\nprint(f\"Processed: {result.processed}\")\nprint(f\"Skipped: {result.skipped}\")\n</code></pre> <p>Cost Estimate</p> <p>Processing costs approximately $0.50 USD for 50 papers via the Gemini API.</p>"},{"location":"quickstart/#3-fetch-citation-data","title":"3. Fetch Citation Data","text":"<pre><code># Enrich with OpenAlex citation counts and references\nmapper.update_citations()\n</code></pre>"},{"location":"quickstart/#4-search-your-corpus","title":"4. Search Your Corpus","text":"<pre><code># Semantic search (finds by meaning, not keywords)\nresults = mapper.search_corpus(\n    query=\"influence of weak ties on information spread\",\n    semantic=True,\n    limit=5\n)\n\nfor r in results:\n    print(f\"[{r['match_score']:.2f}] {r['title']} ({r['year']})\")\n</code></pre>"},{"location":"quickstart/#5-synthesize-answers","title":"5. Synthesize Answers","text":"<pre><code># Ask a research question\u2014get a cited answer\nanswer = mapper.synthesize_answer(\n    \"What factors influence information spread in social networks?\"\n)\nprint(answer)\n</code></pre>"},{"location":"quickstart/#6-validate-hypotheses","title":"6. Validate Hypotheses","text":"<pre><code># Test a claim against your corpus\nresult = mapper.validate_hypothesis(\n    \"Strong ties are more effective than weak ties for spreading information.\"\n)\n\nprint(f\"Verdict: {result['verdict']}\")  # SUPPORTED, CONTRADICTED, MIXED, or NOVEL\nprint(result['explanation'])\n</code></pre>"},{"location":"quickstart/#cli-quick-reference","title":"CLI Quick Reference","text":"<p>Literature Mapper also offers a complete command-line interface:</p> <pre><code># Process PDFs\nliterature-mapper process ./my_research --recursive\n\n# Fetch citations\nliterature-mapper citations ./my_research\n\n# Synthesize an answer\nliterature-mapper synthesize ./my_research \"What is the impact of X on Y?\"\n\n# Find missing papers (ghost hunting)\nliterature-mapper ghosts ./my_research --mode bibliographic\n\n# Export for Gephi\nliterature-mapper viz ./my_research -o graph.gexf --mode semantic\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Full Vignette \u2014 Comprehensive walkthrough of all features</li> <li>CLI Reference \u2014 Complete command documentation</li> <li>API Reference \u2014 Method signatures and parameters</li> </ul>"},{"location":"vignette/citations/","title":"Citation &amp; Influence Analysis","text":"<p>Enrich your corpus with citation data from OpenAlex and identify influential papers.</p>"},{"location":"vignette/citations/#openalex-enrichment","title":"OpenAlex Enrichment","text":"<p>Fetch citation counts and reference lists for each paper:</p> <pre><code># Fetch/update citation data\n# Optionally provide email for faster rate limits\nmapper.update_citations()  # email=\"you@example.com\"\n</code></pre> CLI Equivalent <pre><code>literature-mapper citations ./my_research --email you@example.com\n</code></pre> <p>About OpenAlex</p> <p>OpenAlex is a free, open catalog of scholarly works. Literature Mapper matches your papers by DOI first, then by title. No API key required, but we respect rate limits.</p>"},{"location":"vignette/citations/#citation-metrics","title":"Citation Metrics","text":"Metric Description Use <code>citation_count</code> Total citations Overall influence <code>citations_per_year</code> Citations \u00f7 years since publication Rising influence (controls for age)"},{"location":"vignette/citations/#most-cited-papers","title":"Most Cited Papers","text":"<pre><code>papers_df = mapper.get_all_analyses()\npapers_with_cites = papers_df[papers_df['citation_count'].notna()]\n\n# Top by raw citations\nprint(\"Most Cited (raw):\")\nfor _, row in papers_with_cites.nlargest(3, \"citation_count\").iterrows():\n    print(f\"  {row['citation_count']:,} citations\")\n    print(f\"  {row['authors']} ({row['year']})\")\n    print(f\"  {row['title'][:60]}...\\n\")\n\n# Top by citations per year (normalized)\nprint(\"Most Cited (normalized):\")\nfor _, row in papers_with_cites.nlargest(3, \"citations_per_year\").iterrows():\n    print(f\"  {row['citations_per_year']:.1f} citations/year\")\n    print(f\"  {row['authors']} ({row['year']})\")\n    print(f\"  {row['title'][:60]}...\\n\")\n</code></pre>"},{"location":"vignette/citations/#top-authors-concepts","title":"Top Authors &amp; Concepts","text":"<p>Identify the most prolific researchers and frequent themes:</p> <pre><code>from literature_mapper.analysis import CorpusAnalyzer\n\nanalyzer = CorpusAnalyzer(CORPUS_PATH)\n\nprint(\"Prolific Authors:\")\nfor _, row in analyzer.get_top_authors(limit=5).iterrows():\n    print(f\"  {row['paper_count']}\u00d7 {row['author']}\")\n\nprint(\"\\nFrequent Concepts:\")\nfor _, row in analyzer.get_top_concepts(limit=5).iterrows():\n    print(f\"  {row['paper_count']}\u00d7 {row['concept']}\")\n</code></pre>"},{"location":"vignette/citations/#finding-hub-papers","title":"Finding Hub Papers","text":"<p>Hub papers are those most frequently cited by other papers within your corpus:</p> <pre><code>hubs = analyzer.find_hub_papers(limit=5)\n\nfor _, row in hubs.iterrows():\n    print(f\"  {row['title'][:50]}...\")\n    print(f\"    Referenced by {row['internal_citations']} corpus papers\")\n</code></pre> CLI Equivalent <pre><code>literature-mapper hubs ./my_research --limit 10\n</code></pre>"},{"location":"vignette/export/","title":"Export &amp; Visualization","text":"<p>Export to CSV for analysis, GEXF for network visualization in Gephi.</p>"},{"location":"vignette/export/#csv-export","title":"CSV Export","text":"<p>Export all paper metadata and analyses to a CSV file:</p> <pre><code>mapper.export_to_csv(\"corpus_export.csv\")\n</code></pre> <p>The CSV includes:</p> <ul> <li>Paper metadata (title, authors, year, journal)</li> <li>AI-extracted fields (core argument, methodology, key findings)</li> <li>Citation metrics (if fetched)</li> <li>Concept associations</li> </ul>"},{"location":"vignette/export/#gexf-export-gephi","title":"GEXF Export (Gephi)","text":"<p>Export your corpus as a network graph for visualization in Gephi:</p> <pre><code>from literature_mapper.viz import export_to_gexf\n\nexport_to_gexf(\n    corpus_path=str(CORPUS_PATH),\n    output_path=\"network_semantic.gexf\",\n    threshold=0.05,\n    mode='semantic'\n)\n</code></pre>"},{"location":"vignette/export/#export-modes","title":"Export Modes","text":"Mode Description Best For <code>semantic</code> Full knowledge graph Understanding argument structure <code>authors</code> Co-authorship network Research communities <code>concepts</code> Topic co-occurrence Topic landscape <code>river</code> Concepts with temporal data Dynamic network visualization <code>similarity</code> Paper similarity (Jaccard) Finding related papers"},{"location":"vignette/export/#mode-details","title":"Mode Details","text":""},{"location":"vignette/export/#semantic-default","title":"Semantic (Default)","text":"<p>The full knowledge graph with papers, concepts, findings, methods, and limitations as nodes. Edges represent relationships like SUPPORTS, CONTRADICTS, EXTENDS.</p> <pre><code>export_to_gexf(CORPUS_PATH, \"semantic.gexf\", mode=\"semantic\")\n</code></pre>"},{"location":"vignette/export/#co-authorship","title":"Co-Authorship","text":"<p>Network of authors weighted by shared publications:</p> <pre><code>export_to_gexf(CORPUS_PATH, \"authors.gexf\", mode=\"authors\")\n</code></pre> <p>Use Case</p> <p>Identify \"invisible colleges\"\u2014clusters of researchers who frequently collaborate.</p>"},{"location":"vignette/export/#concept-co-occurrence","title":"Concept Co-occurrence","text":"<p>Network of concepts that appear together in papers:</p> <pre><code>export_to_gexf(CORPUS_PATH, \"concepts.gexf\", mode=\"concepts\")\n</code></pre>"},{"location":"vignette/export/#river-temporal-concepts","title":"River (Temporal Concepts)","text":"<p>Same as <code>concepts</code>, but adds a <code>start</code> year attribute for dynamic visualizations:</p> <pre><code>export_to_gexf(CORPUS_PATH, \"river.gexf\", mode=\"river\")\n</code></pre> <p>Use Case</p> <p>Create ThemeRiver-style visualizations in Gephi showing concept evolution.</p>"},{"location":"vignette/export/#paper-similarity","title":"Paper Similarity","text":"<p>Papers connected by shared concepts (Jaccard similarity):</p> <pre><code>export_to_gexf(CORPUS_PATH, \"similarity.gexf\", mode=\"similarity\", threshold=0.3)\n</code></pre> <p>The <code>threshold</code> parameter controls minimum similarity for an edge (0.0\u20131.0).</p>"},{"location":"vignette/export/#cli-usage","title":"CLI Usage","text":"<pre><code># Default: Semantic knowledge graph\nliterature-mapper viz ./my_research -o graph.gexf\n\n# Co-authorship network\nliterature-mapper viz ./my_research -o authors.gexf --mode authors\n\n# Concept co-occurrence\nliterature-mapper viz ./my_research -o concepts.gexf --mode concepts\n\n# Paper similarity with threshold\nliterature-mapper viz ./my_research -o similar.gexf --mode similarity --threshold 0.3\n</code></pre>"},{"location":"vignette/export/#gephi-workflow","title":"Gephi Workflow","text":"<ol> <li>Open the <code>.gexf</code> file in Gephi</li> <li>Run layout (e.g., ForceAtlas2)</li> <li>Partition by node type or community</li> <li>Size nodes by degree or citation count</li> <li>Export as PNG, SVG, or PDF</li> </ol>"},{"location":"vignette/genealogy/","title":"Intellectual Genealogy","text":"<p>Build a graph of how arguments relate\u2014extensions, challenges, and syntheses across papers.</p>"},{"location":"vignette/genealogy/#what-is-intellectual-genealogy","title":"What is Intellectual Genealogy?","text":"<p>Intellectual genealogy maps how papers relate to each other beyond simple citations:</p> Relation Meaning <code>EXTENDS</code> Builds directly on prior work <code>CHALLENGES</code> Disputes or critiques findings <code>BUILDS_ON</code> Uses as theoretical foundation <code>SYNTHESIZES</code> Combines multiple perspectives"},{"location":"vignette/genealogy/#building-genealogy","title":"Building Genealogy","text":"<p>This makes LLM calls to infer relationships between papers. Results persist to the database.</p> <pre><code># Build once; results are stored in intellectual_edges table\nresult = mapper.build_genealogy(verbose=True)\n\nprint(f\"Analyzed: {result['analyzed']} papers\")\nprint(f\"Relationships found: {result['relationships']}\")\n</code></pre> <p>Time Estimate</p> <p>Approximately 20-30 seconds per paper. For a 50-paper corpus, expect ~25 minutes.</p>"},{"location":"vignette/genealogy/#finding-contradictions","title":"Finding Contradictions","text":"<p>Identify intellectual debates\u2014papers that challenge each other:</p> <pre><code>import textwrap\n\ndebates = mapper.find_contradictions()\n\nif not debates.empty:\n    print(\"INTELLECTUAL DEBATES\\n\")\n\n    for _, row in debates.head(3).iterrows():\n        print(f\"\u250c\u2500 {row['paper_a'][:60]}\")\n        print(f\"\u2502     \u2193 CHALLENGES\")\n        print(f\"\u2514\u2500 {row['paper_b'][:60]}\")\n        print(f\"\\n   Confidence: {row['confidence']:.0%}\")\n\n        if row.get('evidence'):\n            print(f\"\\n   Evidence:\")\n            for line in textwrap.wrap(row['evidence'], width=70):\n                print(f\"   {line}\")\n        print()\n</code></pre>"},{"location":"vignette/genealogy/#argument-evolution","title":"Argument Evolution","text":"<p>Trace how a concept's treatment evolved over time:</p> <pre><code>evolution = mapper.get_argument_evolution(concept=\"weak ties\")\n\nif not evolution.empty:\n    for _, row in evolution.head(5).iterrows():\n        print(f\"{row['source_year']} {row['source_title'][:40]}...\")\n        print(f\"    \u2514\u2500 {row['relation']} \u2192 {row['target_title'][:40]}... ({row['target_year']})\\n\")\n</code></pre>"},{"location":"vignette/genealogy/#example-output","title":"Example Output","text":"<pre><code>1985 Economic Action and Social Structur...\n    \u2514\u2500 BUILDS_ON \u2192 Social Resources and Strength of Ti... (1981)\n\n2004 Structural Holes and Good Ideas...\n    \u2514\u2500 EXTENDS \u2192 Social Resources and Strength of Ti... (1981)\n</code></pre>"},{"location":"vignette/genealogy/#filtering-by-concept","title":"Filtering by Concept","text":"<p>Find debates specifically about a concept:</p> <pre><code># Find contradictions related to \"homophily\"\nhomophily_debates = mapper.find_contradictions(concept=\"homophily\")\n</code></pre>"},{"location":"vignette/genealogy/#use-cases","title":"Use Cases","text":"Goal Method Find debates in the field <code>find_contradictions()</code> Trace idea development <code>get_argument_evolution(concept)</code> Map theoretical lineages Export genealogy edges to Gephi Identify synthesis works Filter for <code>SYNTHESIZES</code> relations"},{"location":"vignette/ghosts/","title":"Gap Detection (Ghost Hunting)","text":"<p>Find frequently-cited papers and authors that are missing from your corpus\u2014structural gaps in your literature review.</p>"},{"location":"vignette/ghosts/#what-is-ghost-hunting","title":"What is Ghost Hunting?","text":"<p>Ghost hunting uses citation data to identify works that are referenced by your corpus but not included in it. This reveals:</p> <ul> <li>Seminal works you may have overlooked</li> <li>Key researchers whose voices are missing</li> <li>Intellectual foundations of your field</li> </ul> <p>Prerequisite</p> <p>Ghost hunting requires running <code>update_citations()</code> first to fetch OpenAlex data.</p>"},{"location":"vignette/ghosts/#ghost-hunting-modes","title":"Ghost Hunting Modes","text":"Mode What It Finds <code>bibliographic</code> Papers cited by your corpus but not included <code>authors</code> Researchers cited frequently but not represented"},{"location":"vignette/ghosts/#bibliographic-ghosts","title":"Bibliographic Ghosts","text":"<p>Find missing papers that are frequently cited by your corpus:</p> <pre><code>from literature_mapper.ghosts import GhostHunter\n\nhunter = GhostHunter(mapper)\n\n# Find papers cited by at least 3 corpus papers\nghosts = hunter.find_bibliographic_ghosts(threshold=3)\n\nif not ghosts.empty:\n    print(f\"Found {len(ghosts)} missing papers:\\n\")\n    for _, row in ghosts.head(5).iterrows():\n        print(f\"  [{row['citation_count']:2d}\u00d7] {row['title'][:50]}...\")\n        print(f\"       {row['author']} ({row['year']})\")\nelse:\n    print(\"No gaps found (run update_citations() first)\")\n</code></pre>"},{"location":"vignette/ghosts/#interpreting-results","title":"Interpreting Results","text":"Column Meaning <code>citation_count</code> Number of corpus papers citing this work <code>title</code> Title of the missing paper <code>author</code> First author or author list <code>year</code> Publication year <p>Actionable Gap Analysis</p> <p>Papers with high citation counts (cited by many corpus papers) are strong candidates to add to your literature review.</p>"},{"location":"vignette/ghosts/#missing-authors","title":"Missing Authors","text":"<p>Find researchers who are frequently cited but don't have papers in your corpus:</p> <pre><code># Find authors cited by at least 5 papers\nmissing = hunter.find_missing_authors(threshold=5)\n\nif not missing.empty:\n    print(\"Frequently cited authors not in corpus:\\n\")\n    for _, row in missing.head(5).iterrows():\n        print(f\"  {row['author']} (cited by {row['cited_by_papers']} papers)\")\n</code></pre>"},{"location":"vignette/ghosts/#cli-usage","title":"CLI Usage","text":"<pre><code># Find missing papers (threshold: cited by at least 3 corpus papers)\nliterature-mapper ghosts ./my_research --mode bibliographic --threshold 3\n\n# Find missing authors (threshold: cited by at least 5 corpus papers)  \nliterature-mapper ghosts ./my_research --mode authors --threshold 5\n</code></pre>"},{"location":"vignette/search/","title":"Search &amp; Retrieval","text":"<p>Find evidence in your corpus using keyword matching or semantic search by meaning.</p>"},{"location":"vignette/search/#keyword-search","title":"Keyword Search","text":"<p>Case-insensitive substring match on specific columns:</p> <pre><code># Find papers using survey methodology\nsurvey_papers = mapper.search_papers(column='methodology', query='survey')\n\nprint(f\"Found {len(survey_papers)} papers using survey methodology:\")\nfor _, row in survey_papers.head(3).iterrows():\n    print(f\"  \u2022 {row['title'][:60]}... ({row['year']})\")\n</code></pre>"},{"location":"vignette/search/#semantic-search","title":"Semantic Search","text":"<p>Semantic search uses embedding vectors to find content by meaning, not exact keywords.</p> <pre><code>results = mapper.search_corpus(\n    query=\"influence of social ties on information diffusion\",\n    semantic=True,\n    limit=5\n)\n\nfor r in results:\n    print(f\"[{r['match_score']:.3f}] {r['title']} ({r['year']})\")\n</code></pre> <p>When to Use Semantic Search</p> <p>Use semantic search when you're looking for conceptually related content that may not use your exact terminology. For example, searching for \"network effects\" might find papers discussing \"social contagion\" or \"peer influence.\"</p>"},{"location":"vignette/search/#enhanced-retrieval","title":"Enhanced Retrieval","text":"<p>Enhanced mode adds two powerful features:</p> Feature Description MMR Reranking Maximal Marginal Relevance ensures diverse results, not just the most similar Consensus Grouping Identifies when multiple papers make the same claim <pre><code>enhanced_results = mapper.search_corpus(\n    query=\"social contagion vs homophily\",\n    semantic=True,\n    use_enhanced=True,\n    node_types=[\"finding\", \"limitation\", \"method\", \"hypothesis\"],\n    limit=5\n)\n\nfor r in enhanced_results:\n    print(f\"[{r['match_score']:.2f}] {r['title']}...\")\n    print(f\"  Type: {r['node_type']}\")\n    print(f\"  Context: {r['match_context'][:100]}...\\n\")\n</code></pre>"},{"location":"vignette/search/#filtering-by-node-type","title":"Filtering by Node Type","text":"<p>Control which knowledge graph node types appear in results:</p> Node Type What It Captures <code>paper</code> Paper titles and abstracts <code>finding</code> Key results and claims <code>method</code> Research methodologies <code>limitation</code> Acknowledged weaknesses <code>hypothesis</code> Theoretical propositions"},{"location":"vignette/search/#year-range-filtering","title":"Year-Range Filtering","text":"<p>Scope any search to specific time periods:</p> <pre><code># Only retrieve evidence from 2010-2020\nrecent_results = mapper.search_corpus(\n    query=\"network centrality\",\n    semantic=True,\n    min_year=2010,\n    max_year=2020,\n    limit=5\n)\n\nfor r in recent_results:\n    print(f\"{r['year']}: {r['title']}\")\n</code></pre> <p>This is especially useful for:</p> <ul> <li>Comparing historical vs. recent perspectives</li> <li>Focusing on foundational works only</li> <li>Analyzing how discourse changed over time</li> </ul>"},{"location":"vignette/setup/","title":"Setup &amp; Understanding Your Corpus","text":"<p>Initialize Literature Mapper, process PDFs, and explore the database structure.</p>"},{"location":"vignette/setup/#prerequisites","title":"Prerequisites","text":"Requirement Details Python 3.10 or newer API Key <code>GEMINI_API_KEY</code> environment variable (get one) Corpus A folder containing PDF files Install <code>pip install literature-mapper</code> <p>Cost Estimate</p> <p>Processing costs approximately $0.50 USD for 50 papers via the Gemini API.</p>"},{"location":"vignette/setup/#initialization","title":"Initialization","text":"<pre><code>import os\nfrom pathlib import Path\nfrom literature_mapper import LiteratureMapper\n\n# Verify API key is set\nif not os.getenv('GEMINI_API_KEY'):\n    raise EnvironmentError(\"GEMINI_API_KEY not found\")\n\n# Define your corpus location\nCORPUS_PATH = Path(\"./my_research\")\n\n# Initialize (creates corpus.db if needed)\nmapper = LiteratureMapper(\n    corpus_path=str(CORPUS_PATH),\n    model_name=\"gemini-3-flash-preview\"\n)\n</code></pre>"},{"location":"vignette/setup/#processing-pdfs","title":"Processing PDFs","text":"<p>Processing is incremental\u2014previously processed PDFs are skipped automatically.</p> <pre><code>result = mapper.process_new_papers(recursive=True)\n\nprint(f\"Processed: {result.processed}\")\nprint(f\"Skipped (already in DB): {result.skipped}\")\nprint(f\"Failed: {result.failed}\")\n</code></pre> Common Issues <ul> <li>Scanned PDFs without OCR \u2192 No extractable text</li> <li>Password-protected files \u2192 Extraction fails</li> <li>Corrupted PDF structure \u2192 Partial or no data</li> </ul> CLI Equivalent <pre><code>literature-mapper process ./my_research --recursive\n</code></pre>"},{"location":"vignette/setup/#database-schema","title":"Database Schema","text":"<p>Literature Mapper stores everything in SQLite (<code>corpus.db</code>). Key tables:</p> Table Description Key Columns <code>papers</code> Core metadata <code>id</code>, <code>title</code>, <code>year</code>, <code>core_argument</code>, <code>methodology</code> <code>authors</code> Unique author names <code>id</code>, <code>name</code>, <code>canonical_name</code> <code>concepts</code> Extracted key terms <code>id</code>, <code>name</code>, <code>canonical_name</code> <code>paper_authors</code> Many-to-many link <code>paper_id</code>, <code>author_id</code> <code>paper_concepts</code> Many-to-many link <code>paper_id</code>, <code>concept_id</code> <code>kg_nodes</code> Knowledge graph nodes <code>id</code>, <code>type</code>, <code>label</code>, <code>vector</code> (embedding) <code>kg_edges</code> Relationships <code>source_id</code>, <code>target_id</code>, <code>relation</code> <code>citations</code> OpenAlex data <code>paper_id</code>, <code>cited_doi</code>, <code>cited_title</code> <code>intellectual_edges</code> Genealogy relationships <code>source_paper_id</code>, <code>target_paper_id</code>, <code>relation_type</code>"},{"location":"vignette/setup/#corpus-statistics","title":"Corpus Statistics","text":"<pre><code>stats = mapper.get_statistics()\n\nprint(f\"Papers:   {stats.total_papers}\")\nprint(f\"Authors:  {stats.total_authors}\")\nprint(f\"Concepts: {stats.total_concepts}\")\n</code></pre>"},{"location":"vignette/setup/#viewing-all-papers","title":"Viewing All Papers","text":"<pre><code>import pandas as pd\n\npapers_df = mapper.get_all_analyses()\npapers_df[['title', 'year', 'authors', 'journal']].head()\n</code></pre>"},{"location":"vignette/setup/#knowledge-graph-structure","title":"Knowledge Graph Structure","text":"<p>The KG contains typed nodes with semantic edges:</p> Node Type Description <code>paper</code> The paper itself <code>author</code> Paper authors <code>finding</code> Key results or claims <code>method</code> Research methods <code>concept</code> Important terms <code>limitation</code> Acknowledged weaknesses <code>hypothesis</code> Proposed theories"},{"location":"vignette/setup/#inspecting-node-distribution","title":"Inspecting Node Distribution","text":"<pre><code>from literature_mapper.database import get_db_session, KGNode, KGEdge\nfrom sqlalchemy import func\n\nwith get_db_session(CORPUS_PATH) as session:\n    node_counts = (\n        session.query(KGNode.type, func.count(KGNode.id))\n        .group_by(KGNode.type)\n        .order_by(func.count(KGNode.id).desc())\n        .all()\n    )\n    edge_count = session.query(KGEdge).count()\n\nprint(\"Node Types:\")\nfor node_type, count in node_counts:\n    print(f\"  {node_type:15s} {count:&gt;5}\")\nprint(f\"\\nTotal edges: {edge_count:,}\")\n</code></pre>"},{"location":"vignette/setup/#temporal-distribution","title":"Temporal Distribution","text":"<p>Visualize when papers in your corpus were published:</p> <pre><code>import matplotlib.pyplot as plt\nfrom literature_mapper.analysis import CorpusAnalyzer\n\nanalyzer = CorpusAnalyzer(CORPUS_PATH)\nyear_dist = analyzer.get_year_distribution()\n\nif not year_dist.empty:\n    fig, ax = plt.subplots(figsize=(10, 3))\n    ax.bar(year_dist['year'], year_dist['count'], color='steelblue')\n    ax.set_xlabel('Year')\n    ax.set_ylabel('Papers')\n    ax.set_title('Publication Timeline')\n    plt.show()\n</code></pre>"},{"location":"vignette/synthesis/","title":"AI Synthesis Agents","text":"<p>Get corpus-grounded answers with citations; validate hypotheses against evidence.</p>"},{"location":"vignette/synthesis/#overview","title":"Overview","text":"<p>Literature Mapper includes two specialized RAG (Retrieval-Augmented Generation) agents:</p> Agent Purpose Method Argument Agent Answer research questions <code>synthesize_answer()</code> Validation Agent Test claims against evidence <code>validate_hypothesis()</code> <p>Key Feature: Bounded Synthesis</p> <p>These agents synthesize answers using only your corpus\u2014no hallucinated citations, no external knowledge bleed. Every claim is grounded in papers you've processed.</p>"},{"location":"vignette/synthesis/#research-synthesis","title":"Research Synthesis","text":"<p>Ask a research question and get a comprehensive, cited answer:</p> <pre><code>import textwrap\n\nquestion = \"What factors influence information spread in social networks?\"\n\nanswer = mapper.synthesize_answer(question, limit=15)\nprint(textwrap.fill(answer, width=100))\n</code></pre>"},{"location":"vignette/synthesis/#how-it-works","title":"How It Works","text":"<ol> <li>Retrieves relevant KG nodes using semantic search</li> <li>Builds context from findings, methods, and limitations</li> <li>Generates a synthesis with inline citations <code>[Author et al., Year]</code></li> <li>All citations reference papers in your corpus</li> </ol>"},{"location":"vignette/synthesis/#hypothesis-validation","title":"Hypothesis Validation","text":"<p>Test a claim against your corpus evidence:</p> <pre><code>hypothesis = \"Strong social ties are more effective than weak ties for spreading novel information.\"\n\nresult = mapper.validate_hypothesis(hypothesis)\n\nprint(f\"Verdict: {result['verdict']}\")\nprint(textwrap.fill(result['explanation'], width=100))\n</code></pre>"},{"location":"vignette/synthesis/#verdict-types","title":"Verdict Types","text":"Verdict Meaning <code>SUPPORTED</code> Corpus evidence aligns with the hypothesis <code>CONTRADICTED</code> Corpus evidence opposes the hypothesis <code>MIXED</code> Both supporting and contradicting evidence exists <code>NOVEL</code> No relevant evidence found (may be original or out of scope)"},{"location":"vignette/synthesis/#temporal-scoping","title":"Temporal Scoping","text":"<p>Compare how perspectives changed over time:</p> <pre><code>question = \"What are the main debates about network structure and information diffusion?\"\n\n# Early period\nearly = mapper.synthesize_answer(question, year_range=(1998, 2008), limit=10)\nprint(\"=== EARLY PERIOD (1998-2008) ===\")\nprint(textwrap.fill(early, width=100))\n\n# Recent period\nrecent = mapper.synthesize_answer(question, year_range=(2015, 2024), limit=10)\nprint(\"\\n=== RECENT PERIOD (2015-2024) ===\")\nprint(textwrap.fill(recent, width=100))\n</code></pre>"},{"location":"vignette/synthesis/#corpus-boundary-enforcement","title":"Corpus Boundary Enforcement","text":"<p>The agent refuses to answer questions outside the corpus domain:</p> <pre><code># Test with an irrelevant query\noff_topic = mapper.synthesize_answer(\"How does soil salinity affect crop yield?\")\n# Returns: \"Based on the provided literature corpus, there is currently \n#           no relevant evidence available...\"\n</code></pre> <pre><code># Test an irrelevant hypothesis\nresult = mapper.validate_hypothesis(\n    \"Glacier retreat in the Himalayas accelerates groundwater depletion.\"\n)\nprint(result['verdict'])  # \"NOVEL\"\n</code></pre> <p>Why This Matters</p> <p>Bounded synthesis prevents the common RAG failure mode where LLMs confidently cite papers that don't exist or make claims unsupported by your actual sources.</p>"},{"location":"vignette/synthesis/#cli-usage","title":"CLI Usage","text":"<pre><code># Synthesize an answer\nliterature-mapper synthesize ./my_research \"What is the impact of X on Y?\"\n\n# Validate a hypothesis\nliterature-mapper validate ./my_research \"X causes Y.\"\n</code></pre>"},{"location":"vignette/temporal/","title":"Temporal Analysis","text":"<p>Track when concepts emerged, detect trends, and identify revivals in your corpus.</p>"},{"location":"vignette/temporal/#concept-timeline","title":"Concept Timeline","text":"<p>See when key concepts first appeared and which papers introduced them within your corpus:</p> <pre><code>timeline = mapper.get_concept_timeline(top_n=5)\n\nprint(f\"{'Concept':&lt;25} {'First':&gt;6} {'Peak':&gt;6} {'Papers':&gt;7}\")\nprint(\"-\" * 50)\nfor _, row in timeline.iterrows():\n    print(f\"{row['concept'][:24]:&lt;25} {row['first_year']:&gt;6} {row['peak_year']:&gt;6} {row['total_papers']:&gt;7}\")\n</code></pre>"},{"location":"vignette/temporal/#tracing-a-specific-concept","title":"Tracing a Specific Concept","text":"<pre><code># Timeline for a specific concept\nspecific = mapper.get_concept_timeline(\"homophily\")\n\nif not specific.empty:\n    for _, row in specific.iterrows():\n        print(f\"{row['concept']}: First appeared {row['first_year']}\")\n        print(f\"  Peak year: {row['peak_year']}\")\n        print(f\"  Introduced by: {row['introduced_by']}\")\n</code></pre>"},{"location":"vignette/temporal/#trend-detection","title":"Trend Detection","text":"<p>After computing temporal statistics, identify rising and declining concepts:</p> <pre><code># First, compute temporal stats\nmapper.compute_temporal_stats()\n\n# Then view trends\nrising = mapper.get_trending_concepts(direction=\"rising\", limit=5)\ndeclining = mapper.get_trending_concepts(direction=\"declining\", limit=5)\n</code></pre>"},{"location":"vignette/temporal/#how-trends-are-computed","title":"How Trends Are Computed","text":"Metric Description Trend Slope Linear regression on papers/year mentioning the concept Positive slope Concept is \"Rising\" (appearing in more papers over time) Negative slope Concept is \"Declining\" (appearing in fewer papers over time)"},{"location":"vignette/temporal/#era-detection-revivals","title":"Era Detection (Revivals)","text":"<p>Detect concepts that disappeared for a period and then reappeared\u2014\"revivals\":</p> <pre><code>eras = mapper.detect_concept_eras(gap=5)  # 5-year gap threshold\n\nfor _, row in eras.iterrows():\n    print(f\"{row['concept']}: {row['first_era']} \u2192 gap \u2192 {row['second_era']}\")\n</code></pre> <p>Use Case</p> <p>Era detection is useful for finding forgotten methods that were later revived, or concepts that experienced a renaissance.</p>"},{"location":"vignette/temporal/#year-range-filtering-in-search","title":"Year-Range Filtering in Search","text":"<p>Scope any search or synthesis to specific time periods:</p> <pre><code># Only retrieve evidence from 2010-2020\nrecent_results = mapper.search_corpus(\n    query=\"network centrality\",\n    semantic=True,\n    min_year=2010,\n    max_year=2020,\n    limit=5\n)\n\nfor r in recent_results:\n    print(f\"{r['year']}: {r['title']}\")\n</code></pre>"},{"location":"vignette/temporal/#cli-usage","title":"CLI Usage","text":"<pre><code># Compute temporal stats (run this first!)\nliterature-mapper temporal ./my_research\n\n# View rising/declining concepts\nliterature-mapper trends ./my_research --direction rising\nliterature-mapper trends ./my_research --direction declining\n\n# Analyze a specific concept's trajectory\nliterature-mapper trajectory \"homophily\" ./my_research\n\n# Detect concept eras (revivals after gaps)\nliterature-mapper eras ./my_research --gap 5\n</code></pre>"}]}